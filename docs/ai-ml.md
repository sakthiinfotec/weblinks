### AI -> ML -> ANN -> DL

#### [Activation Functions](https://www.v7labs.com/blog/neural-networks-activation-functions)
- Linear: Linear means something related to a line. All the linear equations are used to construct a line. Composition of linear functions will give linear result
- Non-Linear: A non-linear equation is such which does not form a straight line. It looks like a curve in a graph and has a variable slope value. Eg. Used to classify images


#### Popular Activation Functions
- [Rectified Linear Unit (ReLU)](https://www.kaggle.com/code/dansbecker/rectified-linear-units-relu-in-deep-learning/notebook): is an activation function that introduces the property of non-linearity to a deep learning model and solves the vanishing gradients issue. "It interprets the positive part of its argument. It is one of the most popular activation functions in deep learning. The function returns 0 if it receives any negative input, but for any positive value  x
  it returns that value back. So it can be written as f(x)=max(0,x).
- [Sigmoid](): This function takes any real value as input and outputs values in the range of 0 to 1.  The larger the input (more positive), the closer the output value will be to 1.0, whereas the smaller the input (more negative), the closer the output will be to 0.0
- Softmax
  
